{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f06867",
   "metadata": {},
   "source": [
    "# LLM Agent Framework: Intelligent Customer Routing\n",
    "\n",
    "This project implements an automated customer service routing system. It uses LLMs to classify incoming user queries and direct them to specialized agents (e.g., FAQ or Order Status).\n",
    "\n",
    "##  Goal\n",
    "To build a high-performance routing framework and evaluate which LLM provides the best balance of **accuracy**, **speed**, and **reliability**.\n",
    "\n",
    "##  System Architecture\n",
    "* **LLM Router:** Classifies queries into categories: `order_status`, `billing`, `technical_support`, `product_info`, and `general`.\n",
    "* **Multi-Model Support:** Integrated with **Llama-3.3 (via Groq)**, and **Gemini-1.5-Flash** (using `.env` for secure API management).\n",
    "* **Evaluation Engine:** A custom `LLMRouterEvaluator` class that benchmarks models on a ground-truth dataset.\n",
    "\n",
    "\n",
    "\n",
    "## Performance Benchmark (Key Results)\n",
    "All tested models achieved **100% accuracy** on the test set, but latency varied significantly:\n",
    "\n",
    "| Metric | Llama-3.3 (Groq) | Gemini-1.5-Flash |\n",
    "| :--- | :--- | :--- |\n",
    "| **Accuracy** | 100% | 100% |\n",
    "| **Avg. Latency** | **0.255s**  | 0.511s |\n",
    "| **Reliability** | High | Medium |\n",
    "\n",
    "**Conclusion:** **Llama-3.3 on Groq** is the recommended model for production due to its ultra-low latency and consistent JSON formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368ecc5",
   "metadata": {},
   "source": [
    "## Downloading libraries â€“ run these if needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn langgraph openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05521dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8efe9c",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a21ae6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List, Literal\n",
    "from dataclasses import dataclass\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import TypedDict\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f69c85c",
   "metadata": {},
   "source": [
    "## Define the state and the result object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e8964ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RoutingResult:\n",
    "    route: Literal[\"order_status\", \"product_info\", \"technical_support\",\"billing\",\"general\"]\n",
    "    confidence: float\n",
    "    method: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5926a03",
   "metadata": {},
   "source": [
    "## Loading API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0683f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All clients initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Groq \n",
    "groq_client = OpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\", \n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "# 2. Gemini\n",
    "gemini_client = OpenAI(\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\" All clients initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3948570",
   "metadata": {},
   "source": [
    "## LLM Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "baba037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_based_routing_llama(query: str) -> RoutingResult:\n",
    "    \"\"\"Route using LLM analysis\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following customer service query and classify it into exactly one category.\n",
    "    \n",
    "    Query: \"{query}\"\n",
    "    \n",
    "    Categories:\n",
    "    - order_status: Questions about order tracking, delivery, shipping status\n",
    "    - product_info: Questions about product specifications, availability, features\n",
    "    - technical_support: Technical issues, troubleshooting, bugs, problems\n",
    "    - billing: Payment, refund, billing, invoice questions\n",
    "    - general: General questions or anything that doesn't fit other categories\n",
    "    \n",
    "    Respond JSON format: {{\"route\": \"\", \"confidence\": 1, \"method\": \"llm\"}}\n",
    "    \"\"\"\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies customer service queries. Respond ONLY with JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=100,\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Get content and clean it\n",
    "    response_content = response.choices[0].message.content\n",
    "    \n",
    "    # Handle markdown backticks if they exist\n",
    "    response_content = response_content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    # Parse and return\n",
    "    response_data = json.loads(response_content)\n",
    "    return RoutingResult(**response_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f21c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_based_routing_gemini(query: str) -> RoutingResult:\n",
    "    \"\"\"\"Route using LLM analysis\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following customer service query and classify it into exactly one category.\n",
    "    \n",
    "    Query: \"{query}\"\n",
    "    \n",
    "    Categories:\n",
    "    - order_status: Questions about order tracking, delivery, shipping status\n",
    "    - product_info: Questions about product specifications, availability, features\n",
    "    - technical_support: Technical issues, troubleshooting, bugs, problems\n",
    "    - billing: Payment, refund, billing, invoice questions\n",
    "    - general: General questions or anything that doesn't fit other categories\n",
    "    \n",
    "    Respond JSON format: {{\"route\": \"\", \"confidence\": 1, \"method\": \"llm\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = gemini_client.chat.completions.create(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "      # Get content and clean it\n",
    "    response_content = response.choices[0].message.content\n",
    "    \n",
    "    # Handle markdown backticks if they exist\n",
    "    response_content = response_content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    # Parse and return\n",
    "    response_data = json.loads(response_content)\n",
    "    return RoutingResult(**response_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0976949d",
   "metadata": {},
   "source": [
    "## Testing the prompts before we actually test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d62efb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoutingResult(route='technical_support', confidence=1, method='llm')\n"
     ]
    }
   ],
   "source": [
    "print(llm_based_routing_gemini(\"I have an issue with the checkout page?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dbc1e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoutingResult(route='technical_support', confidence=1, method='llm')\n"
     ]
    }
   ],
   "source": [
    "print(llm_based_routing_llama(\"I have an issue with the checkout page?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30296b7",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d74110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = [\n",
    "    {\"query\": \"Where is my package ORD123?\", \"expected\": \"order_status\"},\n",
    "    {\"query\": \"How do I return a broken item?\", \"expected\": \"general\"}, # Or FAQ\n",
    "    {\"query\": \"My screen is flickering when I open the app\", \"expected\": \"technical_support\"},\n",
    "    {\"query\": \"Can I pay with PayPal?\", \"expected\": \"billing\"},\n",
    "    {\"query\": \"Tell me about your latest laptop specs\", \"expected\": \"product_info\"},\n",
    "    {\"query\": \"I was charged twice for my last month\", \"expected\": \"billing\"},\n",
    "    {\"query\": \"Why is the website so slow today?\", \"expected\": \"technical_support\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0b85780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMRouterEvaluator:\n",
    "    def __init__(self, routing_functions: dict):\n",
    "        self.routing_functions = routing_functions\n",
    "        self.results = []\n",
    "\n",
    "    def run_evaluation(self, dataset):\n",
    "        for model_name, routing_fn in self.routing_functions.items():\n",
    "            print(f\"Testing {model_name}...\")\n",
    "            for item in dataset:\n",
    "                start_time = time.time()\n",
    "                try:\n",
    "                    # Run the routing logic\n",
    "                    result = routing_fn(item['query'])\n",
    "                    latency = time.time() - start_time\n",
    "                    \n",
    "                    self.results.append({\n",
    "                        \"timestamp\": datetime.now().isoformat(),\n",
    "                        \"model\": model_name,\n",
    "                        \"query\": item['query'],\n",
    "                        \"expected\": item['expected'],\n",
    "                        \"actual\": result.route,\n",
    "                        \"correct\": result.route == item['expected'],\n",
    "                        \"latency\": latency,\n",
    "                        \"confidence\": result.confidence\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {model_name} on query '{item['query']}': {e}\")\n",
    "\n",
    "    def get_summary(self):\n",
    "        df = pd.DataFrame(self.results)\n",
    "        # Calculate metrics per model\n",
    "        summary = df.groupby(\"model\").agg(\n",
    "            accuracy=(\"correct\", \"mean\"),\n",
    "            avg_latency=(\"latency\", \"mean\"),\n",
    "            avg_confidence=(\"confidence\", \"mean\")\n",
    "        ).reset_index()\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6105d0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Llama-3.3...\n",
      "Testing gemini-2.5-flash-lite...\n",
      "                   model  accuracy  avg_latency  avg_confidence\n",
      "0              Llama-3.3       1.0     0.255023             1.0\n",
      "1  gemini-2.5-flash-lite       1.0     0.511483             1.0\n"
     ]
    }
   ],
   "source": [
    "models_to_test = {\n",
    "    \"Llama-3.3\": llm_based_routing_llama,\n",
    "    \"gemini-2.5-flash-lite\": llm_based_routing_gemini\n",
    "}\n",
    "\n",
    "evaluator = LLMRouterEvaluator(models_to_test)\n",
    "evaluator.run_evaluation(test_dataset)\n",
    "\n",
    "# Display the comparison table\n",
    "summary_table = evaluator.get_summary()\n",
    "print(summary_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4455224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model</th>\n",
       "      <th>query</th>\n",
       "      <th>expected</th>\n",
       "      <th>actual</th>\n",
       "      <th>correct</th>\n",
       "      <th>latency</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-18T13:21:00.439493</td>\n",
       "      <td>Llama-3.3</td>\n",
       "      <td>Where is my package ORD123?</td>\n",
       "      <td>order_status</td>\n",
       "      <td>order_status</td>\n",
       "      <td>True</td>\n",
       "      <td>0.732694</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-18T13:21:00.590648</td>\n",
       "      <td>Llama-3.3</td>\n",
       "      <td>How do I return a broken item?</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>True</td>\n",
       "      <td>0.151130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-18T13:21:00.773899</td>\n",
       "      <td>Llama-3.3</td>\n",
       "      <td>My screen is flickering when I open the app</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>True</td>\n",
       "      <td>0.183237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-18T13:21:00.906291</td>\n",
       "      <td>Llama-3.3</td>\n",
       "      <td>Can I pay with PayPal?</td>\n",
       "      <td>billing</td>\n",
       "      <td>billing</td>\n",
       "      <td>True</td>\n",
       "      <td>0.132373</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-18T13:21:01.051795</td>\n",
       "      <td>Llama-3.3</td>\n",
       "      <td>Tell me about your latest laptop specs</td>\n",
       "      <td>product_info</td>\n",
       "      <td>product_info</td>\n",
       "      <td>True</td>\n",
       "      <td>0.145481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-12-18T13:21:01.197820</td>\n",
       "      <td>Llama-3.3</td>\n",
       "      <td>I was charged twice for my last month</td>\n",
       "      <td>billing</td>\n",
       "      <td>billing</td>\n",
       "      <td>True</td>\n",
       "      <td>0.146004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-12-18T13:21:01.492075</td>\n",
       "      <td>Llama-3.3</td>\n",
       "      <td>Why is the website so slow today?</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>True</td>\n",
       "      <td>0.294242</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-12-18T13:21:02.040797</td>\n",
       "      <td>gemini-2.5-flash-lite</td>\n",
       "      <td>Where is my package ORD123?</td>\n",
       "      <td>order_status</td>\n",
       "      <td>order_status</td>\n",
       "      <td>True</td>\n",
       "      <td>0.548641</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-12-18T13:21:02.615263</td>\n",
       "      <td>gemini-2.5-flash-lite</td>\n",
       "      <td>How do I return a broken item?</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>True</td>\n",
       "      <td>0.574450</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-12-18T13:21:03.154853</td>\n",
       "      <td>gemini-2.5-flash-lite</td>\n",
       "      <td>My screen is flickering when I open the app</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>True</td>\n",
       "      <td>0.539570</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-12-18T13:21:03.745303</td>\n",
       "      <td>gemini-2.5-flash-lite</td>\n",
       "      <td>Can I pay with PayPal?</td>\n",
       "      <td>billing</td>\n",
       "      <td>billing</td>\n",
       "      <td>True</td>\n",
       "      <td>0.590424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-12-18T13:21:04.256915</td>\n",
       "      <td>gemini-2.5-flash-lite</td>\n",
       "      <td>Tell me about your latest laptop specs</td>\n",
       "      <td>product_info</td>\n",
       "      <td>product_info</td>\n",
       "      <td>True</td>\n",
       "      <td>0.511591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-12-18T13:21:04.664890</td>\n",
       "      <td>gemini-2.5-flash-lite</td>\n",
       "      <td>I was charged twice for my last month</td>\n",
       "      <td>billing</td>\n",
       "      <td>billing</td>\n",
       "      <td>True</td>\n",
       "      <td>0.407957</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-12-18T13:21:05.072657</td>\n",
       "      <td>gemini-2.5-flash-lite</td>\n",
       "      <td>Why is the website so slow today?</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>True</td>\n",
       "      <td>0.407746</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp                  model  \\\n",
       "0   2025-12-18T13:21:00.439493              Llama-3.3   \n",
       "1   2025-12-18T13:21:00.590648              Llama-3.3   \n",
       "2   2025-12-18T13:21:00.773899              Llama-3.3   \n",
       "3   2025-12-18T13:21:00.906291              Llama-3.3   \n",
       "4   2025-12-18T13:21:01.051795              Llama-3.3   \n",
       "5   2025-12-18T13:21:01.197820              Llama-3.3   \n",
       "6   2025-12-18T13:21:01.492075              Llama-3.3   \n",
       "7   2025-12-18T13:21:02.040797  gemini-2.5-flash-lite   \n",
       "8   2025-12-18T13:21:02.615263  gemini-2.5-flash-lite   \n",
       "9   2025-12-18T13:21:03.154853  gemini-2.5-flash-lite   \n",
       "10  2025-12-18T13:21:03.745303  gemini-2.5-flash-lite   \n",
       "11  2025-12-18T13:21:04.256915  gemini-2.5-flash-lite   \n",
       "12  2025-12-18T13:21:04.664890  gemini-2.5-flash-lite   \n",
       "13  2025-12-18T13:21:05.072657  gemini-2.5-flash-lite   \n",
       "\n",
       "                                          query           expected  \\\n",
       "0                   Where is my package ORD123?       order_status   \n",
       "1                How do I return a broken item?            general   \n",
       "2   My screen is flickering when I open the app  technical_support   \n",
       "3                        Can I pay with PayPal?            billing   \n",
       "4        Tell me about your latest laptop specs       product_info   \n",
       "5         I was charged twice for my last month            billing   \n",
       "6             Why is the website so slow today?  technical_support   \n",
       "7                   Where is my package ORD123?       order_status   \n",
       "8                How do I return a broken item?            general   \n",
       "9   My screen is flickering when I open the app  technical_support   \n",
       "10                       Can I pay with PayPal?            billing   \n",
       "11       Tell me about your latest laptop specs       product_info   \n",
       "12        I was charged twice for my last month            billing   \n",
       "13            Why is the website so slow today?  technical_support   \n",
       "\n",
       "               actual  correct   latency  confidence  \n",
       "0        order_status     True  0.732694           1  \n",
       "1             general     True  0.151130           1  \n",
       "2   technical_support     True  0.183237           1  \n",
       "3             billing     True  0.132373           1  \n",
       "4        product_info     True  0.145481           1  \n",
       "5             billing     True  0.146004           1  \n",
       "6   technical_support     True  0.294242           1  \n",
       "7        order_status     True  0.548641           1  \n",
       "8             general     True  0.574450           1  \n",
       "9   technical_support     True  0.539570           1  \n",
       "10            billing     True  0.590424           1  \n",
       "11       product_info     True  0.511591           1  \n",
       "12            billing     True  0.407957           1  \n",
       "13  technical_support     True  0.407746           1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = pd.DataFrame(evaluator.results)\n",
    "\n",
    "full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec562309",
   "metadata": {},
   "source": [
    "## Conclusion & Model Recommendation\n",
    "\n",
    "After evaluating the performance of **Llama-3.3-70b (via Groq)** and **Gemini-2.5-Flash-Lite (via Google)** across our test suite, the following conclusions were drawn:\n",
    "\n",
    "###  The Winner: Llama-3.3-70b (on Groq)\n",
    "**Llama-3.3** is the recommended model for this routing framework due to its superior balance of speed and reliability.\n",
    "\n",
    "\n",
    "###  Performance Summary\n",
    "| Metric | Llama-3.3 (Groq) | Gemini-2.5-Flash-Lite |\n",
    "| :--- | :--- | :--- |\n",
    "| **Accuracy** | 100% | 100% |\n",
    "| **Avg. Latency** | **0.255s**  | 0.511s |\n",
    "| **Reliability** | High  | Medium  |\n",
    "\n",
    "### Final Recommendation\n",
    "For production-level customer service routing, **Llama-3.3 on Groq** is the best choice. It provides the low-latency performance required for routing logic while maintaining the same intelligence level as proprietary models. **Gemini-2.5-Flash-Lite** remains a high-quality alternative if your infrastructure is already built within the Google Cloud/Firebase ecosystem, provided that rate limits are managed via exponential backoff."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
