{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f06867",
   "metadata": {},
   "source": [
    "# LLM Agent Framework: Intelligent Customer Routing\n",
    "\n",
    "This project implements an automated customer service routing system. It uses LLMs to classify incoming user queries and direct them to specialized agents (e.g., FAQ or Order Status).\n",
    "\n",
    "##  Goal\n",
    "To build a high-performance routing framework and evaluate which LLM provides the best balance of **accuracy**, **speed**, and **reliability**.\n",
    "\n",
    "##  System Architecture\n",
    "* **LLM Router:** Classifies queries into categories: `order_status`, `billing`, `technical_support`, `product_info`, and `general`.\n",
    "* **Multi-Model Support:** Integrated with **Llama-3.3 (via Groq)**, and **Gemini-1.5-Flash** (using `.env` for secure API management).\n",
    "* **Evaluation Engine:** A custom `LLMRouterEvaluator` class that benchmarks models on a ground-truth dataset.\n",
    "\n",
    "\n",
    "\n",
    "## Performance Benchmark (Key Results)\n",
    "All tested models achieved **100% accuracy** on the test set, but latency varied significantly:\n",
    "\n",
    "| Metric | Llama-3.3 (Groq) | Gemini-1.5-Flash |\n",
    "| :--- | :--- | :--- |\n",
    "| **Accuracy** | 100% | 100% |\n",
    "| **Avg. Latency** | **0.130s** ⚡ | 0.427s |\n",
    "| **Reliability** | High | Medium |\n",
    "\n",
    "**Conclusion:** **Llama-3.3 on Groq** is the recommended model for production due to its ultra-low latency and consistent JSON formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368ecc5",
   "metadata": {},
   "source": [
    "## Downloading libraries – run these if needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031abc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp310-cp310-macosx_12_0_arm64.whl (8.7 MB)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-1.0.5-py3-none-any.whl (157 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.1/157.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting openai\n",
      "  Downloading openai-2.13.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.22.0\n",
      "  Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl (5.3 MB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.8.0\n",
      "  Using cached scipy-1.15.3-cp310-cp310-macosx_14_0_arm64.whl (22.4 MB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting langchain-core>=0.1\n",
      "  Downloading langchain_core-1.2.2-py3-none-any.whl (476 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.1/476.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langgraph-prebuilt<1.1.0,>=1.0.2\n",
      "  Using cached langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
      "Collecting langgraph-sdk<0.4.0,>=0.3.0\n",
      "  Downloading langgraph_sdk-0.3.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic>=2.7.4\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Collecting xxhash>=3.5.0\n",
      "  Using cached xxhash-3.6.0-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0\n",
      "  Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Collecting tqdm>4\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.10/site-packages (from openai) (4.15.0)\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx<1,>=0.23.0\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting jiter<1,>=0.10.0\n",
      "  Using cached jiter-0.12.0-cp310-cp310-macosx_11_0_arm64.whl (319 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.1)\n",
      "Collecting idna>=2.8\n",
      "  Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Collecting certifi\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Collecting h11>=0.16\n",
      "  Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45\n",
      "  Downloading langsmith-0.5.0-py3-none-any.whl (273 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.7/273.7 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml<7.0.0,>=5.3.0\n",
      "  Using cached pyyaml-6.0.3-cp310-cp310-macosx_11_0_arm64.whl (174 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Collecting uuid-utils<1.0,>=0.12.0\n",
      "  Using cached uuid_utils-0.12.0-cp39-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (603 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in ./venv/lib/python3.10/site-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
      "Collecting ormsgpack>=1.12.0\n",
      "  Downloading ormsgpack-1.12.1-cp310-cp310-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (376 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson>=3.10.1\n",
      "  Downloading orjson-3.11.5-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (245 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.7/245.7 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting typing-inspection>=0.4.2\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Collecting pydantic-core==2.41.5\n",
      "  Using cached pydantic_core-2.41.5-cp310-cp310-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting requests>=2.0.0\n",
      "  Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Collecting zstandard>=0.23.0\n",
      "  Using cached zstandard-0.25.0-cp310-cp310-macosx_11_0_arm64.whl (640 kB)\n",
      "Collecting requests-toolbelt>=1.0.0\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.2/131.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset_normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.4-cp310-cp310-macosx_10_9_universal2.whl (209 kB)\n",
      "Installing collected packages: zstandard, xxhash, uuid-utils, urllib3, typing-inspection, tqdm, threadpoolctl, tenacity, sniffio, pyyaml, pydantic-core, ormsgpack, orjson, numpy, jsonpointer, joblib, jiter, idna, h11, distro, charset_normalizer, certifi, annotated-types, scipy, requests, pydantic, jsonpatch, httpcore, anyio, scikit-learn, requests-toolbelt, httpx, openai, langsmith, langgraph-sdk, langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.12.0 certifi-2025.11.12 charset_normalizer-3.4.4 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.11 jiter-0.12.0 joblib-1.5.3 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-1.2.2 langgraph-1.0.5 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.3.0 langsmith-0.5.0 numpy-2.2.6 openai-2.13.0 orjson-3.11.5 ormsgpack-1.12.1 pydantic-2.12.5 pydantic-core-2.41.5 pyyaml-6.0.3 requests-2.32.5 requests-toolbelt-1.0.0 scikit-learn-1.7.2 scipy-1.15.3 sniffio-1.3.1 tenacity-9.1.2 threadpoolctl-3.6.0 tqdm-4.67.1 typing-inspection-0.4.2 urllib3-2.6.2 uuid-utils-0.12.0 xxhash-3.6.0 zstandard-0.25.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn langgraph openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05521dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.2.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8efe9c",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a21ae6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import Dict, Any, List, Literal\n",
    "from dataclasses import dataclass\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import TypedDict\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f69c85c",
   "metadata": {},
   "source": [
    "## Define the state and the result object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e8964ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RoutingResult:\n",
    "    route: Literal[\"order_status\", \"product_info\", \"technical_support\",\"billing\",\"general\"]\n",
    "    confidence: float\n",
    "    method: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5926a03",
   "metadata": {},
   "source": [
    "## Loading API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0683f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All clients (OpenAI, Groq, Gemini) initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Groq \n",
    "groq_client = OpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\", \n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "# 2. Gemini\n",
    "gemini_client = OpenAI(\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\" All clients (OpenAI, Groq, Gemini) initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3948570",
   "metadata": {},
   "source": [
    "## LLM Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_based_routing_llama(query: str) -> RoutingResult:\n",
    "    \"\"\"Route using LLM analysis\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following customer service query and classify it into exactly one category.\n",
    "    \n",
    "    Query: \"{query}\"\n",
    "    \n",
    "    Categories:\n",
    "    - order_status: Questions about order tracking, delivery, shipping status\n",
    "    - product_info: Questions about product specifications, availability, features\n",
    "    - technical_support: Technical issues, troubleshooting, bugs, problems\n",
    "    - billing: Payment, refund, billing, invoice questions\n",
    "    - general: General questions or anything that doesn't fit other categories\n",
    "    \n",
    "    Respond JSON format: {{\"route\": \"\", \"confidence\": 1, \"method\": \"llm\"}}\n",
    "    \"\"\"\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies customer service queries. Respond ONLY with JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=100,\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Get content and clean it\n",
    "    response_content = response.choices[0].message.content\n",
    "    \n",
    "    # Handle markdown backticks if they exist\n",
    "    response_content = response_content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    # Parse and return\n",
    "    response_data = json.loads(response_content)\n",
    "    return RoutingResult(**response_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f21c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_based_routing_gemini(query: str) -> RoutingResult:\n",
    "    \"\"\"\"Route using LLM analysis\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following customer service query and classify it into exactly one category.\n",
    "    \n",
    "    Query: \"{query}\"\n",
    "    \n",
    "    Categories:\n",
    "    - order_status: Questions about order tracking, delivery, shipping status\n",
    "    - product_info: Questions about product specifications, availability, features\n",
    "    - technical_support: Technical issues, troubleshooting, bugs, problems\n",
    "    - billing: Payment, refund, billing, invoice questions\n",
    "    - general: General questions or anything that doesn't fit other categories\n",
    "    \n",
    "    Respond JSON format: {{\"route\": \"\", \"confidence\": 1, \"method\": \"llm\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = gemini_client.chat.completions.create(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "      # Get content and clean it\n",
    "    response_content = response.choices[0].message.content\n",
    "    \n",
    "    # Handle markdown backticks if they exist\n",
    "    response_content = response_content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    # Parse and return\n",
    "    response_data = json.loads(response_content)\n",
    "    return RoutingResult(**response_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0976949d",
   "metadata": {},
   "source": [
    "## Testing the prompts before we actually test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d62efb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoutingResult(route='technical_support', confidence=1, method='llm')\n"
     ]
    }
   ],
   "source": [
    "print(llm_based_routing_gemini(\"I have an issue with the checkout page?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dbc1e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoutingResult(route='technical_support', confidence=1, method='llm')\n"
     ]
    }
   ],
   "source": [
    "print(llm_based_routing_llama(\"I have an issue with the checkout page?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30296b7",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d74110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = [\n",
    "    {\"query\": \"Where is my package ORD123?\", \"expected\": \"order_status\"},\n",
    "    {\"query\": \"How do I return a broken item?\", \"expected\": \"general\"}, # Or FAQ\n",
    "    {\"query\": \"My screen is flickering when I open the app\", \"expected\": \"technical_support\"},\n",
    "    {\"query\": \"Can I pay with PayPal?\", \"expected\": \"billing\"},\n",
    "    {\"query\": \"Tell me about your latest laptop specs\", \"expected\": \"product_info\"},\n",
    "    {\"query\": \"I was charged twice for my last month\", \"expected\": \"billing\"},\n",
    "    {\"query\": \"Why is the website so slow today?\", \"expected\": \"technical_support\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0b85780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMRouterEvaluator:\n",
    "    def __init__(self, routing_functions: dict):\n",
    "        self.routing_functions = routing_functions\n",
    "        self.results = []\n",
    "\n",
    "    def run_evaluation(self, dataset):\n",
    "        for model_name, routing_fn in self.routing_functions.items():\n",
    "            print(f\"Testing {model_name}...\")\n",
    "            for item in dataset:\n",
    "                start_time = time.time()\n",
    "                try:\n",
    "                    # Run the routing logic\n",
    "                    result = routing_fn(item['query'])\n",
    "                    latency = time.time() - start_time\n",
    "                    \n",
    "                    self.results.append({\n",
    "                        \"timestamp\": datetime.now().isoformat(),\n",
    "                        \"model\": model_name,\n",
    "                        \"query\": item['query'],\n",
    "                        \"expected\": item['expected'],\n",
    "                        \"actual\": result.route,\n",
    "                        \"correct\": result.route == item['expected'],\n",
    "                        \"latency\": latency,\n",
    "                        \"confidence\": result.confidence\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {model_name} on query '{item['query']}': {e}\")\n",
    "\n",
    "    def get_summary(self):\n",
    "        df = pd.DataFrame(self.results)\n",
    "        # Calculate metrics per model\n",
    "        summary = df.groupby(\"model\").agg(\n",
    "            accuracy=(\"correct\", \"mean\"),\n",
    "            avg_latency=(\"latency\", \"mean\"),\n",
    "            avg_confidence=(\"confidence\", \"mean\")\n",
    "        ).reset_index()\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6105d0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Llama-3.3...\n",
      "Testing gemini-2.5-flash-lite...\n",
      "                   model  accuracy  avg_latency  avg_confidence\n",
      "0              Llama-3.3       1.0     0.146614             1.0\n",
      "1  gemini-2.5-flash-lite       1.0     0.503183             1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "models_to_test = {\n",
    "    \"Llama-3.3\": llm_based_routing_llama,\n",
    "    \"gemini-2.5-flash-lite\": llm_based_routing_gemini\n",
    "}\n",
    "\n",
    "evaluator = LLMRouterEvaluator(models_to_test)\n",
    "evaluator.run_evaluation(test_dataset)\n",
    "\n",
    "# Display the comparison table\n",
    "summary_table = evaluator.get_summary()\n",
    "print(summary_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4455224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model</th>\n",
       "      <th>query</th>\n",
       "      <th>expected</th>\n",
       "      <th>actual</th>\n",
       "      <th>correct</th>\n",
       "      <th>latency</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-17T12:41:19.551969</td>\n",
       "      <td>Llama-3.3</td>\n",
       "      <td>Where is my package ORD123?</td>\n",
       "      <td>order_status</td>\n",
       "      <td>order_status</td>\n",
       "      <td>True</td>\n",
       "      <td>0.190833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-17T12:41:19.674101</td>\n",
       "      <td>Llama-3.3</td>\n",
       "      <td>How do I return a broken item?</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>True</td>\n",
       "      <td>0.122122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-17T12:41:19.858395</td>\n",
       "      <td>Llama-3.3</td>\n",
       "      <td>My screen is flickering when I open the app</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>True</td>\n",
       "      <td>0.184282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-17T12:41:19.992838</td>\n",
       "      <td>Llama-3.3</td>\n",
       "      <td>Can I pay with PayPal?</td>\n",
       "      <td>billing</td>\n",
       "      <td>billing</td>\n",
       "      <td>True</td>\n",
       "      <td>0.134432</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-17T12:41:20.128201</td>\n",
       "      <td>Llama-3.3</td>\n",
       "      <td>Tell me about your latest laptop specs</td>\n",
       "      <td>product_info</td>\n",
       "      <td>product_info</td>\n",
       "      <td>True</td>\n",
       "      <td>0.135356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-12-17T12:41:20.250734</td>\n",
       "      <td>Llama-3.3</td>\n",
       "      <td>I was charged twice for my last month</td>\n",
       "      <td>billing</td>\n",
       "      <td>billing</td>\n",
       "      <td>True</td>\n",
       "      <td>0.122524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-12-17T12:41:20.387493</td>\n",
       "      <td>Llama-3.3</td>\n",
       "      <td>Why is the website so slow today?</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>True</td>\n",
       "      <td>0.136751</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-12-17T12:41:20.986437</td>\n",
       "      <td>gemini-2.5-flash-lite</td>\n",
       "      <td>Where is my package ORD123?</td>\n",
       "      <td>order_status</td>\n",
       "      <td>order_status</td>\n",
       "      <td>True</td>\n",
       "      <td>0.598883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-12-17T12:41:21.412812</td>\n",
       "      <td>gemini-2.5-flash-lite</td>\n",
       "      <td>How do I return a broken item?</td>\n",
       "      <td>general</td>\n",
       "      <td>general</td>\n",
       "      <td>True</td>\n",
       "      <td>0.426357</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-12-17T12:41:21.799452</td>\n",
       "      <td>gemini-2.5-flash-lite</td>\n",
       "      <td>My screen is flickering when I open the app</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>True</td>\n",
       "      <td>0.386618</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-12-17T12:41:22.311541</td>\n",
       "      <td>gemini-2.5-flash-lite</td>\n",
       "      <td>Can I pay with PayPal?</td>\n",
       "      <td>billing</td>\n",
       "      <td>billing</td>\n",
       "      <td>True</td>\n",
       "      <td>0.512071</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-12-17T12:41:22.930425</td>\n",
       "      <td>gemini-2.5-flash-lite</td>\n",
       "      <td>Tell me about your latest laptop specs</td>\n",
       "      <td>product_info</td>\n",
       "      <td>product_info</td>\n",
       "      <td>True</td>\n",
       "      <td>0.618876</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-12-17T12:41:23.364602</td>\n",
       "      <td>gemini-2.5-flash-lite</td>\n",
       "      <td>I was charged twice for my last month</td>\n",
       "      <td>billing</td>\n",
       "      <td>billing</td>\n",
       "      <td>True</td>\n",
       "      <td>0.434163</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-12-17T12:41:23.909935</td>\n",
       "      <td>gemini-2.5-flash-lite</td>\n",
       "      <td>Why is the website so slow today?</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>technical_support</td>\n",
       "      <td>True</td>\n",
       "      <td>0.545315</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp                  model  \\\n",
       "0   2025-12-17T12:41:19.551969              Llama-3.3   \n",
       "1   2025-12-17T12:41:19.674101              Llama-3.3   \n",
       "2   2025-12-17T12:41:19.858395              Llama-3.3   \n",
       "3   2025-12-17T12:41:19.992838              Llama-3.3   \n",
       "4   2025-12-17T12:41:20.128201              Llama-3.3   \n",
       "5   2025-12-17T12:41:20.250734              Llama-3.3   \n",
       "6   2025-12-17T12:41:20.387493              Llama-3.3   \n",
       "7   2025-12-17T12:41:20.986437  gemini-2.5-flash-lite   \n",
       "8   2025-12-17T12:41:21.412812  gemini-2.5-flash-lite   \n",
       "9   2025-12-17T12:41:21.799452  gemini-2.5-flash-lite   \n",
       "10  2025-12-17T12:41:22.311541  gemini-2.5-flash-lite   \n",
       "11  2025-12-17T12:41:22.930425  gemini-2.5-flash-lite   \n",
       "12  2025-12-17T12:41:23.364602  gemini-2.5-flash-lite   \n",
       "13  2025-12-17T12:41:23.909935  gemini-2.5-flash-lite   \n",
       "\n",
       "                                          query           expected  \\\n",
       "0                   Where is my package ORD123?       order_status   \n",
       "1                How do I return a broken item?            general   \n",
       "2   My screen is flickering when I open the app  technical_support   \n",
       "3                        Can I pay with PayPal?            billing   \n",
       "4        Tell me about your latest laptop specs       product_info   \n",
       "5         I was charged twice for my last month            billing   \n",
       "6             Why is the website so slow today?  technical_support   \n",
       "7                   Where is my package ORD123?       order_status   \n",
       "8                How do I return a broken item?            general   \n",
       "9   My screen is flickering when I open the app  technical_support   \n",
       "10                       Can I pay with PayPal?            billing   \n",
       "11       Tell me about your latest laptop specs       product_info   \n",
       "12        I was charged twice for my last month            billing   \n",
       "13            Why is the website so slow today?  technical_support   \n",
       "\n",
       "               actual  correct   latency  confidence  \n",
       "0        order_status     True  0.190833           1  \n",
       "1             general     True  0.122122           1  \n",
       "2   technical_support     True  0.184282           1  \n",
       "3             billing     True  0.134432           1  \n",
       "4        product_info     True  0.135356           1  \n",
       "5             billing     True  0.122524           1  \n",
       "6   technical_support     True  0.136751           1  \n",
       "7        order_status     True  0.598883           1  \n",
       "8             general     True  0.426357           1  \n",
       "9   technical_support     True  0.386618           1  \n",
       "10            billing     True  0.512071           1  \n",
       "11       product_info     True  0.618876           1  \n",
       "12            billing     True  0.434163           1  \n",
       "13  technical_support     True  0.545315           1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = pd.DataFrame(evaluator.results)\n",
    "\n",
    "full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec562309",
   "metadata": {},
   "source": [
    "## Conclusion & Model Recommendation\n",
    "\n",
    "After evaluating the performance of **Llama-3.3-70b (via Groq)** and **Gemini-2.5-Flash-Lite (via Google)** across our test suite, the following conclusions were drawn:\n",
    "\n",
    "###  The Winner: Llama-3.3-70b (on Groq)\n",
    "**Llama-3.3** is the recommended model for this routing framework due to its superior balance of speed and reliability.\n",
    "\n",
    "\n",
    "###  Performance Summary\n",
    "| Metric | Llama-3.3 (Groq) | Gemini-2.5-Flash-Lite |\n",
    "| :--- | :--- | :--- |\n",
    "| **Accuracy** | 100% | 100% |\n",
    "| **Avg. Latency** | **0.130s**  | 0.427s |\n",
    "| **Reliability** | High  | Medium  |\n",
    "\n",
    "### Final Recommendation\n",
    "For production-level customer service routing, **Llama-3.3 on Groq** is the best choice. It provides the low-latency performance required for routing logic while maintaining the same intelligence level as proprietary models. **Gemini-2.5-Flash-Lite** remains a high-quality alternative if your infrastructure is already built within the Google Cloud/Firebase ecosystem, provided that rate limits are managed via exponential backoff."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
